---
title: "Statistical Analysis"
author: "Jiayi Shi"
date: "2022-12-09"
output:
  html_document:
    code_folding: hide
    toc: true
    toc_float: true
---

```{r setup, include=FALSE}
library(tidyverse)
library(patchwork)
library(gtsummary)

knitr::opts_chunk$set(
  warning = FALSE, 
  message = FALSE,
  fig.width = 6,
  fig.asp = .6,
  out.width = "90%"
)

theme_set(theme_minimal() + theme(legend.position = "bottom"))

options(
  ggplot2.continuous.colour = "viridis",
  ggplot2.continuous.fill = "viridis"
)

scale_colour_discrete = scale_colour_viridis_d
scale_fill_discrete = scale_fill_viridis_d
```


## Dataset Overview

In this first step, we wrangle the data to be analyzable and provide a brief overview of our dataset and our variables of interest.

```{r}
library(readr)
dt = read_csv("data/inspection_sub_all_date.csv")
df = read_csv("data/inspection_sub_latest_date.csv") %>%
  distinct(camis, .keep_all = TRUE) %>% 
  mutate(price = fct_recode(price, "1" = "$", "2" = "$$", "3" = "$$$", "4" = "$$$$"),
         boro = fct_reorder(boro, rating),
         grade = fct_relevel(grade, "A"))

```

Now, we provide a table summary of all variables involved.

```{r}
df %>% 
  select(boro, grade, price) %>% 
  mutate(boro = fct_infreq(boro)) %>% 
  tbl_summary(
    by = boro,
    missing_text = "(Missing)", 
    statistic = list(
      all_categorical() ~ "{n} ({p}%)"
      )) %>% 
  bold_labels() %>%   
  italicize_levels() 

df %>% 
  select(score,rating, review_num) %>% 
  tbl_summary(
    missing_text = "(Missing)", 
    statistic = list(
      all_continuous() ~ "{mean} ({sd})"
      )) %>% 
  bold_labels() %>%   
  italicize_levels() 
```


We observe that,

* Manhattan has the most restaurants (40% of all restaurants in the city), followed by Brooklyn (28%), Queens (22%), Bronx (6.6%), and Staten Island (3.4%).

* The "score" variable has 100 missing values with mean 18 and standard deviation 14.

* The "grade" variable has 1,399 missing values with most of the restaurants receiving As (70% of all restaurants).

* Each restaurant has average rating 3.78 with standard deviation 0.68.

* Each restaurant has on average 232 reviews with standard deviation 485.

* The most common price range for restaurants is between \$11-\$30 (60%), followed by under \$10 (31%).


## Exploratory Data Analysis

We first perform chi-square tests and proportion test to get a sense of the data that we are dealing with. Specifically, we ask the following questions: 

* is there a relationship between boroughs and restaurant's inspection grades? 

* Is there a relationship between price and restaurant's inspection grades? 

* More specifically, is price range of the restaurant a factor in the inspection grades that a particular restaurant receives?


### Chi-square Test

#### Inspection Grades and Boroughs

We try to determine whether there is a relationship between boroughs and restaurants' inspection grades. Our hypothesis is that there is no difference in the number of restaurants across the five grades across the five boroughs in NYC. We will perform the chi-square test to verify our assumption.

$H0$: the expected number of restaurants in each grades are the same across all boroughs.

$H1$: the expected number of restaurants in each grades are not same across all boroughs.

```{r}
grade_boro = 
  df %>% 
  filter(grade %in% c("A", "B", "C")) %>% 
  count(boro, grade) %>% 
  pivot_wider(
    names_from = "grade",
    values_from = "n") %>%
  replace(is.na(.), 0) 

# Display result.
grade_boro %>% 
  knitr::kable(caption = "Results Table",
               col.names = c("Borough", "A", "B", "C"))
```

```{r}
# Perform chi-square test
grade_boro =  grade_boro %>%  
  data.matrix() %>%
  subset(select = -c(boro))

chisq.test(grade_boro)
```

Interpretation: The result of chi-square shows that p-value is less than 0.05, so we reject the null hypothesis at 95% significant level and conclude that the inspection grades of restaurants are significantly different by boroughs. 


#### Price and Inspection Grade

Next, we focus on whether inspection grade received is different among different groups of price range. 

$H0$: There is no difference in the inspection grade received among the four different groups of price range.

$H1$: There is a difference in the inspecion grade received among the four different groups of price range.

```{r}
price_grade = df %>% 
  select(boro, score, grade, rating, review_num, price) %>%
  drop_na(price, grade) %>% 
  filter(grade %in% c("A", "B", "C")) %>% 
  group_by(price, grade) %>% 
  summarise(n = n()) %>% 
  pivot_wider(
    names_from = grade,
    values_from = n 
  ) %>% 
  replace(is.na(.), 0) %>% 
  mutate(price = fct_recode(price, "<10"="1", "11-30"="2", "31-60"="3", ">60"="4"))

price_grade %>% 
  knitr::kable(caption = "Results Table",
               col.names = c("Price($)", "A", "B", "C"))

price_grade = price_grade %>% 
  data.matrix() %>%
  subset(select = -c(price))

# Perform test
chisq.test(price_grade)
```

Interpretation: The result of chi-square shows that p-value is more than 0.05, so we fail to reject the null hypothesis at 95% significant level and conclude that the inspection grades of restaurants do not differ by price scales of restaurants.


### Proportion Test

Now, we want to see whether receiving grade A is equally common among restaurants of all four price scales. To do this, we will conduct a proportion test.

$H0$: Receiving A is equally common among restaurants fo all four price scales.

$H1$: Receiving A is not equally common among restaurants for all four price scales.

```{r}
total = df %>% 
  group_by(price) %>% 
  summarise(total = n())

n_a = df %>% 
  count(price, grade) %>% 
  filter(grade == "A")

join = left_join(total, n_a) %>% drop_na()
prop.test(join$n, join$total)

#join = join %>% 
 # mutate(prop = n/total) %>% 
 # select(price, prop)
```

Since p-value  0.07 > 0.05, at confidence level of 95%, we fail to reject the null hypothesis, and conclude that we do not have enough evidence to claim that the proportion of restaurants receiving A is different among four datasets.


## Regression model

Now, we perform regression analysis from our data treating "rating" as the response variable of interest.

### Data Exploration and Variable Transformation

The dataset used for regression model contains three categorical variables and three continuous variables. From the plot below, we can see that score and review number is left skewed and rating is right skewed. 

```{r}
top_cuisine = df %>%
  count(cuisine_description) %>% 
  arrange(-n) %>% 
  top_n(4) %>% 
  pull(cuisine_description)

reg_df = df %>% 
  select(rating, boro, cuisine_description, score, review_num, price) %>%
  filter(cuisine_description %in% top_cuisine) %>% 
  filter(grade %in% c("A", "B", "C"))

library("ggplot2")                     
library("GGally")

reg_df %>% 
  ggpairs()+
  theme_bw()

```


#### Data Transformation

We first test the normality assumption of the response variable, "rating". We see from the below histogram that it is negatively skewed. Though sample size is large, to avoid confounding, we need to square-transform the rating variable. Furthermore, after checking the dependent variable review_num, we've found the data to be positively skewed. We remedy the problem by log-transforming it.

```{r}
# Histogram for response variable rating
reg_df %>% 
  ggplot(aes(x = rating))+
  geom_histogram(aes(y = ..density..), bins = 30, color = "black", fill = "grey") +
  geom_density()

# QQ plot
qqnorm(reg_df$rating)
```


Comparing the residual vs. fitted value plot of rating and review_num before and after log transformation of review_num, we see now that the transformed data does not significantly violate the assumptions of linearity, equal error variance, and no outlier assumptions. This is the same as we do log transformation of score.

```{r}
par(mfrow = c(2, 2))
plot(rating ~ review_num, data = reg_df, col = "dodgerblue", pch = 20, cex = 1.5)
review = lm(rating ~ review_num, data = reg_df)
abline(review, col = "darkorange", lwd = 2)
plot(fitted(review), resid(review), col = "dodgerblue",
     pch = 20, cex = 1.5, xlab = "Fitted", ylab = "Residuals")
abline(h = 0, lty = 2, col = "darkorange", lwd = 2)
title(main = "Before transformation")

plot(rating ~ log(review_num), data = reg_df, col = "dodgerblue", pch = 20, cex = 1.5)
review = lm(rating ~ log(review_num), data = reg_df)
abline(review, col = "darkorange", lwd = 2)
plot(fitted(review), resid(review), col = "dodgerblue",
     pch = 20, cex = 1.5, xlab = "Fitted", ylab = "Residuals")
abline(h = 0, lty = 2, col = "darkorange", lwd = 2)
title(main = "After transformation")
```

```{r}
par(mfrow = c(2, 2))
plot(rating ~ score, data = reg_df, col = "dodgerblue", pch = 20, cex = 1.5)
rating_score = lm(rating ~ score, data = reg_df)
abline(rating_score, col = "darkorange", lwd = 2)
plot(fitted(rating_score), resid(rating_score), col = "dodgerblue",
     pch = 20, cex = 1.5, xlab = "Fitted", ylab = "Residuals")
abline(h = 0, lty = 2, col = "darkorange", lwd = 2)
title(main = "Before transformation")

plot(rating ~ log(score+0.1), data = reg_df, col = "dodgerblue", pch = 20, cex = 1.5)
rating_score = lm(rating ~ log(score+0.1), data = reg_df)
abline(rating_score, col = "darkorange", lwd = 2)
plot(fitted(rating_score), resid(rating_score), col = "dodgerblue",
     pch = 20, cex = 1.5, xlab = "Fitted", ylab = "Residuals")
abline(h = 0, lty = 2, col = "darkorange", lwd = 2)
title(main = "After transformation")
```


```{r}
# Log transform review_num, square transform rating
reg_df = reg_df %>% 
  select(rating, everything()) %>% 
  mutate(review_num = log(review_num),
         score = log(score+0.1),
         rating = rating^2)
```


#### Prognosis of Significant Predictors

In this section, we speculate the potentially statistically significant predictors for response variable "rating". We use boxplots to visualize the effects of categorical variables like cuisine type, inspection grade, and price scales; and use scatterplots for continuous distributions like score and review number.

##### Categorical Independent Variables

Firstly, we visualize the effects of categorical variables cuisine type, inspection grade, and price scales on rating. In particular, we have found that rating has some association with cuisine type, inspection grade and price scale.

```{r}
reg_df %>% 
  ggplot(aes(y = rating, x = cuisine_description, color = cuisine_description))+
  geom_boxplot()

reg_df %>%
  ggplot(aes(y = rating, x= grade, color = grade))+
  geom_boxplot()

reg_df %>%
  drop_na(price) %>% 
  mutate(price = fct_recode(price, "<10"="1", "11-30"="2", "31-60"="3", ">60"="4")) %>% 
  ggplot(aes(y = rating, x = price, color = price))+
  geom_boxplot()

reg_df %>%
  mutate(boro = fct_reorder(boro, rating)) %>% 
  ggplot(aes(y = rating, x = boro, color = boro))+
  geom_boxplot()
```

As shown in the boxplots above, when considering the most populated cuisine types, restaurants that serve coffee/tea tend to receive the highest ratings. Furthermore, though restaurants that receive A or B for grade inspection have similar median rating, restaurants receiving C for grade inspection have a slightly lower median rating compared to those that received A or B. Finally, there seems to be an association between price range and rating as higher priced restaurants tend to receive better ratings.

##### Continuous Independent Variables

Now, we consider the effects of continuous independent variables.

#### Interaction and Confounding

The cross lines indicate that borough and cuisine type is a confounder of the relationship between score and rating.

```{r}
p1 = reg_df %>% 
  ggplot(aes(x = score, y = rating, color = cuisine_description))+
  geom_point()+
  geom_smooth(method="lm", se=F, aes(group = cuisine_description, color = cuisine_description))
p2 = reg_df %>% 
  ggplot(aes(x = score, y = rating, color = boro))+
  geom_point()+
  geom_smooth(method="lm", se=F, aes(group = boro, color = boro))

p1+p2
```

The cross lines indicate that borough and cuisine type is a confounder of the relationship between review numbers and rating.

```{r}
p1 = reg_df %>% 
  ggplot(aes(x = review_num, y = rating, color = cuisine_description))+
  geom_point()+
  geom_smooth(method="lm", se=F, aes(group = cuisine_description, color = cuisine_description))
p2 = reg_df %>% 
  ggplot(aes(x = review_num, y = rating, color = boro))+
  geom_point()+
  geom_smooth(method="lm", se=F, aes(group = boro, color = boro))

p1+p2
```

#### MLR Molde selection

```{r}
model = lm(rating ~ score + cuisine_description*review_num + boro*review_num + price, data = reg_df)
summary(model)
```

10-fold cross validation:
```{r}
library(caret)
set.seed(1)
train = trainControl(method = "cv", number = 10)
model = train(rating ~ cuisine_description*score+boro+review_num+price, 
              data = reg_df,
              trControl = train,
              method = 'lm',
              na.action = na.pass)
model$finalModel
print(model)
```

Elastic Net
```{r}
library(glmnet)
library(caret)
set.seed(2)
cv_10 = trainControl(method = "cv", number = 10)
x = model.matrix(~ ., dplyr::select(reg_df, -rating))[,-1]
y = reg_df$rating
elnet_int = train(
  rating ~ . ^ 2, data = reg_df %>% drop_na(),
  method = "glmnet",
  trControl = cv_10,
  tuneLength = 10
)

get_best_result = function(caret_fit) {
  best = which(rownames(caret_fit$results) == rownames(caret_fit$bestTune))
  best_result = caret_fit$results[best, ]
  rownames(best_result) = NULL
  best_result
}

get_best_result(elnet_int)
```

